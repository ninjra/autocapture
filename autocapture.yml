mode:
  mode: "local"
  # Remote mode: set this to your overlay adapter (e.g. tailscale0, wg0). When
  # remote mode is enabled, api.bind_host is derived from this interface.
  overlay_interface: null
  https_enabled: false
  tls_cert_path: null
  tls_key_path: null
  google_oauth_client_id: null
  google_oauth_client_secret: null
  google_allowed_emails: []
api:
  # Remote mode: bind_host is derived from mode.overlay_interface unless you
  # override it.
  bind_host: "127.0.0.1"
  port: 8008
  require_api_key: false
  api_key: null
  rate_limit_rps: 2.0
  rate_limit_burst: 5
  max_page_size: 200
  default_page_size: 50
  max_query_chars: 2000
  max_context_k: 50
security:
  local_unlock_enabled: true
  session_ttl_seconds: 300
  provider: "windows_hello"
templates:
  enabled: true
  log_provenance: true
ui:
  overlay_citations_enabled: false
overlay_tracker:
  enabled: false
  platforms: ["windows"]
  stale_after_hours: 48
  hotness_half_life_minutes: 30
  collectors:
    foreground_enabled: true
    input_enabled: true
    fallback_foreground_poll_ms: 1000
    input_poll_ms: 250
    input_debounce_ms: 500
  ui:
    enabled: true
    dock: "right"
    width_px: 320
    click_through_default: true
    interactive_timeout_seconds: 10
    refresh_ms: 1000
    auto_hide_fullscreen: true
  hotkeys:
    toggle_overlay:
      modifiers: ["ctrl", "shift"]
      key: "O"
    interactive_mode:
      modifiers: ["ctrl", "shift"]
      key: "I"
    project_cycle:
      modifiers: ["ctrl", "shift"]
      key: "P"
    toggle_running:
      modifiers: ["ctrl", "shift"]
      key: "R"
    rename:
      modifiers: ["ctrl", "shift"]
      key: "N"
    snooze:
      modifiers: ["ctrl", "shift"]
      key: "S"
    snooze_minutes: [15, 60, 240]
  policy:
    deny_processes: []
    max_window_title_length: 512
  retention:
    event_days: 14
    event_cap: 200000
  url_plugin:
    enabled: false
    allow_browsers: ["chrome.exe", "msedge.exe"]
    allow_domains: []
    token_rules: []
offline: true
features:
  enable_otel: true
  enable_memory_service_write_hook: false
  enable_memory_service_read_hook: false
privacy:
  cloud_enabled: false
  sanitize_default: true
  extractive_only_default: true
  paused: false
  snooze_until_utc: null
  exclude_monitors: []
  exclude_processes:
    - "1password.exe"
    - "bitwarden.exe"
    - "keepass.exe"
    - "keepassxc.exe"
    - "lastpass.exe"
    - "dashlane.exe"
    - "nordpass.exe"
    - "keeper.exe"
    - "roboform.exe"
    - "authy.exe"
  exclude_window_title_regex:
    - "(?i)\\b(inprivate|incognito)\\b"
    - "(?i)windows security"
    - "(?i)user account control"
    - "(?i)credential(s)?"
  exclude_regions: []
policy:
  enforce_prompt_injection: false
  prompt_injection_warn_threshold: 0.7
  prompt_injection_block_threshold: 0.9
  structured_output_mode: "none"
  max_context_chars: null
  max_evidence_items: null
output:
  format: "text"
  context_pack_format: "json"
  allow_tron_compression: false
cache:
  enabled: false
  path: null
  max_entries: 10000
  ttl_s: 86400
  prune_interval_s: 3600
  redact_on_cloud: true
retrieval:
  graph_adapters:
    graphrag:
      enabled: true
      base_url: "http://127.0.0.1:8020"
      timeout_s: 10.0
      max_results: 20
    hypergraphrag:
      enabled: true
      base_url: "http://127.0.0.1:8020"
      timeout_s: 10.0
      max_results: 20
    hyperrag:
      enabled: true
      base_url: "http://127.0.0.1:8020"
      timeout_s: 10.0
      max_results: 20
memory:
  enabled: true
  api_context_pack_enabled: true
  storage:
    # root_dir uses the platform default unless AUTOCAPTURE_MEMORY_DIR is set.
    db_filename: "memory.sqlite3"
    artifacts_dir: "artifacts"
    snapshots_dir: "snapshots"
    require_fts: true
    snapshot_retention_days: 90
  policy:
    blocked_labels: []
    exclude_patterns: []
    redact_patterns: []
    redact_token: "[REDACTED]"
  spans:
    max_chars: 800
    min_chars: 200
  retrieval:
    enabled: true
    default_k: 8
    max_k: 24
    recency_half_life_days: 30
  compiler:
    max_total_chars: 6000
    max_chars_per_span: 1200
    max_spans: 12
    max_memory_items: 32
  hotness:
    enabled: true
    mode_default: "off" # off|as_of|dynamic
    scope_default: "default"
    half_lives:
      fast_seconds: 3600
      mid_seconds: 21600
      warm_seconds: 86400
      cool_seconds: 604800
    weights:
      fast: 0.4
      mid: 0.3
      warm: 0.2
      cool: 0.1
    thresholds:
      hot: 0.75
      recent: 0.5
      warm: 0.25
      cool: 0.1
    quotas:
      hot: 0.4
      recent: 0.3
      warm: 0.2
      cool: 0.1
    rate_limit:
      enabled: true
      min_interval_ms: 60000
    retention:
      event_max_age_days: 30
      event_max_count: 50000
    allowed_signals:
      manual_touch: ["cli", "api"]
      pin_set: ["cli", "api"]
      pin_unset: ["cli", "api"]
memory_service:
  enabled: false
  bind_host: "127.0.0.1"
  port: 8030
  require_api_key: false
  api_key: null
  base_url: null
  database_url: null
  default_namespace: "default"
  max_body_bytes: 1000000
  request_timeout_s: 10.0
  enable_ingest: true
  enable_query: true
  enable_feedback: true
  enable_rerank: false
  enable_query_embedding: true
  enable_rls: false
  embedder:
    provider: "stub"
    dim: 256
    model_id: "hash-v1"
  reranker:
    provider: "disabled"
    max_window: 50
  retrieval:
    topk_vector: 40
    topk_keyword: 40
    topk_graph: 40
    graph_depth: 2
    graph_max_nodes: 200
    rerank_window: 50
    max_cards: 8
    max_tokens: 1200
    max_per_type: 2
    type_priority: ["decision", "procedure", "fact", "episodic", "glossary"]
  ranking:
    weight_semantic: 0.45
    weight_keyword: 0.25
    weight_graph: 0.1
    weight_recency: 0.1
    weight_importance: 0.05
    weight_trust: 0.05
    weight_rerank: 0.2
    recency_half_life_days: 90
  policy:
    allowed_audiences: ["internal"]
    sensitivity_order: ["low", "medium", "high"]
    reject_person_entities: true
    reject_preferences: true
    pii_patterns: []
    secret_patterns: []
presets:
  active_preset: "privacy_first"
routing:
  capture: "local"
  ocr: "local"
  embedding: "local"
  retrieval: "local"
  reranker: "enabled"
  compressor: "extractive"
  verifier: "rules"
  llm: "gateway"
llm:
  provider: "gateway"
  ollama_url: "http://127.0.0.1:11434"
  ollama_model: "llama3"
  openai_api_key: null
  openai_model: "gpt-4.1-mini"
  timeout_s: 60.0
  retries: 3
  # Repeat the user prompt once (higher input tokens; may improve non-reasoning tasks).
  prompt_repetition: false
llm_governor:
  enabled: true
  min_in_flight: 1
  max_in_flight: 4
  low_pressure_threshold: 0.45
  high_pressure_threshold: 0.85
  adjust_interval_s: 5.0
gateway:
  enabled: true
  bind_host: "127.0.0.1"
  port: 8010
  require_api_key: false
  api_key: null
  internal_token: null
  gpu_max_concurrency: 2
  max_body_bytes: 2000000
  request_timeout_s: 60.0
  upstream_probe_timeout_s: 5.0
  startup_probe: true
graph_service:
  enabled: true
  bind_host: "127.0.0.1"
  port: 8020
  max_events: 50000
  max_results: 200
  require_workers: true
  graphrag_cli: "scripts/graphrag_worker.sh"
  hypergraphrag_cli: "scripts/hypergraphrag_worker.sh"
  hyperrag_cli: "scripts/hyperrag_worker.sh"
  worker_timeout_s: 60.0
model_registry:
  enabled: true
  providers:
    - id: "vllm_gpu_a"
      type: "openai_compatible"
      base_url: "http://127.0.0.1:8001"
      timeout_s: 60.0
      retries: 2
      allow_cloud: false
      max_concurrency: 2
    - id: "vllm_gpu_b"
      type: "openai_compatible"
      base_url: "http://127.0.0.1:8002"
      timeout_s: 60.0
      retries: 2
      allow_cloud: false
      max_concurrency: 1
    - id: "vllm_cpu"
      type: "openai_compatible"
      base_url: "http://127.0.0.1:8003"
      timeout_s: 60.0
      retries: 2
      allow_cloud: false
      max_concurrency: 2
  models:
    - id: "base_fast"
      provider_id: "vllm_gpu_a"
      upstream_model_name: "MODEL_FAST"
      context_tokens: 8192
      runtime:
        device: "cuda"
    - id: "base_long"
      provider_id: "vllm_gpu_b"
      upstream_model_name: "MODEL_LONG"
      context_tokens: 16384
      runtime:
        device: "cuda"
      lmcache_enabled: true
    - id: "base_cpu"
      provider_id: "vllm_cpu"
      upstream_model_name: "MODEL_CPU"
      context_tokens: 8192
      runtime:
        device: "cpu"
    - id: "verifier_small"
      provider_id: "vllm_gpu_a"
      upstream_model_name: "MODEL_VERIFIER"
      context_tokens: 4096
      runtime:
        device: "cuda"
  stages:
    - id: "draft_generate"
      primary_model_id: "base_fast"
      fallback_model_ids: ["base_cpu"]
      sampling:
        temperature: 0.2
      requirements:
        require_json: false
        require_citations: true
      decode:
        strategy: "standard"
      max_attempts: 2
      allow_cloud: false
      repair_on_failure: true
    - id: "final_answer"
      primary_model_id: "base_long"
      fallback_model_ids: ["base_fast", "base_cpu"]
      sampling:
        temperature: 0.2
      requirements:
        require_json: true
        claims_schema: "claims_json_v1"
      decode:
        strategy: "standard"
      max_attempts: 2
      allow_cloud: false
      repair_on_failure: true
    - id: "entailment_judge"
      primary_model_id: "verifier_small"
      fallback_model_ids: ["base_cpu"]
      sampling:
        temperature: 0.0
      requirements:
        require_json: true
      decode:
        strategy: "standard"
      max_attempts: 2
      allow_cloud: false
      repair_on_failure: true
verification:
  claims_enabled: true
  citation_validator:
    max_claims: 20
    max_citations_per_claim: 8
    allow_empty: false
    allow_legacy_evidence_ids: false
    max_line_span: 8
  entailment:
    enabled: true
    judge_stage: "entailment_judge"
    on_contradiction: "block"
    on_nei: "abstain"
    max_attempts: 2
promptops:
  enabled: false
  schedule_cron: "0 6 * * 1"
  sources: []
  github_token: null
  github_repo: "https://github.com/ninjra/promptops"
  # Phase 1 settings (PromptOps)
  max_iterations: 3
  max_llm_attempts_per_prompt: 3
  eval_repeats: 3
  eval_aggregation: "worst_case"
  require_improvement: true
  min_delta_verifier_pass_rate: 0.02
  min_delta_citation_coverage: 0.02
  min_delta_refusal_rate: 0.02
  min_delta_latency_ms: 200.0
  acceptance_tolerance: 0.02
  tolerance_citation_coverage: 0.02
  tolerance_refusal_rate: 0.02
  tolerance_latency_ms: 250.0
  min_verifier_pass_rate: 0.60
  min_citation_coverage: 0.60
  max_refusal_rate: 0.30
  max_mean_latency_ms: 15000.0
  max_source_bytes: 1048576
  max_source_excerpt_chars: 2000
  max_sources: 32
agents:
  enabled: true
  max_pending_jobs: 5000
  nightly_cron: "0 3 * * *"
  answer_agent:
    enabled: true
  vision:
    provider: "ollama"
    model: "llava"
    base_url: null
    api_key: null
    max_jobs_per_hour: 20
    run_only_when_idle: true
    idle_hours_start: 22
    idle_hours_end: 6
enrichment:
  enabled: true
  scan_interval_s: 900.0
  max_events_per_scan: 2000
  window_days: null
  at_risk_hours: 24
threads:
  enabled: true
  max_gap_minutes: 15.0
  app_similarity_threshold: 0.5
  title_similarity_threshold: 0.3
  max_events_per_thread: 100
capture:
  hid:
    min_interval_ms: 500
    idle_grace_ms: 1500
    duplicate_threshold: 0.02
    duplicate_window_s: 10.0
    duplicate_max_items: 16
    duplicate_pixel_threshold: 2.5
    fps_soft_cap: 0.5
    block_fullscreen: true
  fps_min: 0.5
  fps_max: 2.0
  tile_size: 512
  fullscreen_primary: true
  fullscreen_width: 3840
  focus_crop_enabled: true
  focus_crop_size: 512
  focus_crop_reference: "event"
  diff_epsilon: 0.04
  downscale_width: 256
  always_store_fullres: false
  thumbnail_width: 640
  # staging_dir: "%LOCALAPPDATA%/Autocapture/staging"  # optional override
  # data_dir: "%LOCALAPPDATA%/Autocapture/data"       # optional override
  encoder: "nvenc_webp"
  record_video: false
  multi_monitor_enabled: true
  hdr_enabled: false
  layout_mode: "virtual_desktop"
  video_bitrate: "8M"
  video_preset: "p4"
  max_pending: 8000
tracking:
  enabled: true
  db_path: "db/host_events.sqlite"
  queue_maxsize: 20000
  flush_interval_ms: 1000
  foreground_poll_ms: 250
  clipboard_poll_ms: 250
  track_mouse_movement: true
  mouse_move_sample_ms: 50
  enable_clipboard: false
  retention_days: null
ocr:
  queue_maxsize: 4000
  batch_size: 32
  max_latency_s: 900
  engine: "rapidocr-onnxruntime"
  device: "cuda"
  languages: ["en"]
  output_format: "json"
  layout_enabled: true
  paddle_ppstructure_enabled: false
  paddle_ppstructure_model_dir: null
  paddle_ppstructure_use_gpu: false
embed:
  text_model: "BAAI/bge-base-en-v1.5"
  image_model: "google/siglip2-so400m-patch14-384"
  text_batch_size: 256
  image_batch_size: 32
  schedule_cron: "0 */6 * * *"
  use_half_precision: true
reranker:
  enabled: true
  model: "BAAI/bge-reranker-v2-m3"
  top_k: 100
worker:
  # data_dir: "%LOCALAPPDATA%/Autocapture/data"       # optional override
  lease_ms: 60000
  ocr_lease_ms: 60000
  embedding_lease_ms: 60000
  poll_interval_s: 1.0
  ocr_backlog_soft_limit: 5000
  ocr_max_attempts: 5
  ocr_workers: 4
  embed_workers: 1
  embedding_max_attempts: 5
retention:
  video_days: 3
  roi_days: 7
  max_media_gb: 200
  screenshot_ttl_days: 30
storage:
  image_quota_gb: 250
  prune_grace_days: 90
  prune_batch: 3000
database:
  # url: "sqlite:///C:/Path/To/Autocapture/data/autocapture.db"  # optional override
  echo: false
  pool_size: 10
  max_overflow: 10
qdrant:
  enabled: true
  url: "http://127.0.0.1:6333"
  # binary_path: "C:/Path/To/qdrant.exe"  # optional override
  text_collection: "text_spans"
  image_collection: "image_tiles"
  text_vector_size: 768
  image_vector_size: 768
  distance: "Cosine"
  hnsw_ef_construct: 128
  hnsw_m: 16
  search_ef: 64
ffmpeg:
  enabled: true
  require_bundled: true
  relative_path_candidates:
    - "ffmpeg/bin/ffmpeg.exe"
    - "ffmpeg/ffmpeg.exe"
encryption:
  enabled: true
  key_provider: "windows-credential-manager"
  key_name: "autocapture/nas-aes-key"
  chunk_size: 4194304
observability:
  prometheus_bind_host: "127.0.0.1"
  prometheus_port: 9005
  prometheus_port_fallbacks: 10
  prometheus_fail_fast: false
  grafana_url: "http://localhost:3000"
  enable_gpu_stats: true
  telemetry:
    capture_payloads: "none"
    exporter: "none"
    otlp_endpoint: null
    otlp_protocol: "http/protobuf"
    allow_cloud_export: false
    max_attr_len: 128
